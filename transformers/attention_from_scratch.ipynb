{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uxabS_9JLs"
      },
      "source": [
        "# 📖 In‑Depth Introduction to Attention\n",
        "\n",
        "The **attention mechanism** is a core component of the Transformer architecture. Its primary role is to let the model decide **which parts of the input are most relevant** when computing each output representation. Unlike fixed-context methods like convolutions or RNNs, attention enables **dynamic, data-driven interactions** between all input positions.\n",
        "\n",
        "At the heart of attention are three learned projections: **queries**, **keys**, and **values**. For each output token, the model uses a query to compare against all keys in the sequence, generating **content-based weights** that determine how much to incorporate each value into the final representation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Queries, Keys & Values: The Semantics\n",
        "\n",
        "1. **Queries (Q)**  \n",
        "   - Think of a query as **“what I’m looking for.”**  \n",
        "   - For each position in the output, we derive a query vector that encodes the aspects of the input we want to match against.  \n",
        "   - Semantically, a query represents the “question” we ask of all other positions.\n",
        "\n",
        "2. **Keys (K)**  \n",
        "   - A key represents **“what each position has to offer.”**  \n",
        "   - Each input position produces a key vector that describes its content or characteristics.  \n",
        "   - Semantically, a key is like a “descriptor” that the query compares against.\n",
        "\n",
        "3. **Values (V)**  \n",
        "   - A value represents **“what information to bring back.”**  \n",
        "   - Each position also produces a value vector that carries the actual content to be integrated into the output.  \n",
        "   - If a query “attends” strongly to a key, its corresponding value is used to update the representation at the query’s position.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Why Attention Matters\n",
        "\n",
        "- **Dynamic Context**  \n",
        "  - Every output position can attend to every input position.  \n",
        "  - The model learns to focus on the most relevant parts of the sequence—whether they are nearby or far apart.\n",
        "\n",
        "- **Parallelizable**  \n",
        "  - All queries, keys, and values are computed at once for the entire sequence.  \n",
        "  - Unlike RNNs, there’s no enforced sequential dependency, so GPUs/TPUs can process everything in parallel.\n",
        "\n",
        "- **Rich Representations**  \n",
        "  - By mixing information from multiple positions, attention captures long‑range dependencies and nuanced relationships.  \n",
        "  - This leads to more expressive and context‑aware features than fixed‑window or local‑only methods.\n",
        "\n",
        "- **Flexible & General**  \n",
        "  - Attention is the core of models in NLP (machine translation, summarization), vision (Vision Transformer), audio, video, graphs, and beyond.  \n",
        "  - Its fundamental idea—learning content‑based weights—applies across domains and modalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU3gFe-k9Lb1"
      },
      "source": [
        "# 🔍 Single‑Head Self‑Attention: Step‑by‑Step\n",
        "\n",
        "**Core idea**  \n",
        "Each token “looks at” every other token and builds a new representation as a weighted sum of their values.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Input\n",
        "$$\n",
        "X \\;\\in\\; \\mathbb{R}^{B \\times N \\times D},\n",
        "$$\n",
        "where  \n",
        "- $B$ = batch size  \n",
        "- $N$ = sequence length (number of tokens)  \n",
        "- $D$ = embedding dimension  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Linear projections\n",
        "Learn three weight matrices  \n",
        "$$\n",
        "W^Q,\\;W^K,\\;W^V \\;\\in\\; \\mathbb{R}^{D \\times D}\n",
        "$$  \n",
        "and compute:\n",
        "$$\n",
        "Q = X\\,W^Q,\\quad\n",
        "K = X\\,W^K,\\quad\n",
        "V = X\\,W^V\n",
        "$$  \n",
        "All of shape $\\mathbb{R}^{B \\times N \\times D}$.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Compute raw scores\n",
        "$$\n",
        "\\mathrm{Scores} = Q\\,K^\\top\n",
        "\\;\\in\\;\\mathbb{R}^{B \\times N \\times N},\n",
        "$$  \n",
        "where $K^\\top$ denotes transposing the last two dimensions of $K$.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Scale and normalize\n",
        "$$\n",
        "\\mathrm{Scores}_{\\text{scaled}}\n",
        "= \\frac{\\mathrm{Scores}}{\\sqrt{D}}\n",
        "\\quad,\\quad\n",
        "A = \\mathrm{softmax}\\!\\bigl(\\mathrm{Scores}_{\\text{scaled}}\\bigr)\n",
        "\\;\\in\\;\\mathbb{R}^{B \\times N \\times N}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Weighted sum of values\n",
        "$$\n",
        "Z = A\\,V\n",
        "\\;\\in\\;\\mathbb{R}^{B \\times N \\times D},\n",
        "$$  \n",
        "where each output token $z_i = \\sum_{j=1}^N A_{ij}\\,v_j$.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Complexity\n",
        "- **Projections**: $\\mathcal{O}(N\\,D^2)$  \n",
        "- **Attention (dot + weighted sum)**: $\\mathcal{O}(N^2\\,D)$  \n",
        "- **Total**: $\\displaystyle \\mathcal{O}(N^2 D + N D^2)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CvkkgzOM9P2T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.q_proj = nn.Linear(dim, dim)\n",
        "    self.k_proj = nn.Linear(dim, dim)\n",
        "    self.v_proj = nn.Linear(dim, dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Apply single-head self-attention on the input sequence.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (B, N, D), where:\n",
        "            - B = batch size\n",
        "            - N = number of tokens (sequence length or patches)\n",
        "            - D = embedding dimension\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor of shape (B, N, D), where each token's representation\n",
        "                is updated based on its attention to all other tokens.\n",
        "\n",
        "    Steps:\n",
        "        1. Compute queries, keys, and values using linear projections.\n",
        "        2. Compute attention scores using scaled dot-product between queries and keys.\n",
        "        3. Normalize scores with softmax to get attention weights.\n",
        "        4. Compute weighted sum of value vectors based on attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate query, key & value\n",
        "    q = self.q_proj(x)                              # -> (B, N, D)\n",
        "    k = self.k_proj(x)                              # -> (B, N, D)\n",
        "    v = self.v_proj(x)                              # -> (B, N, D)\n",
        "\n",
        "    # Compute attention weights\n",
        "    k_t = k.transpose(1, 2)                         # -> (B, D, N)\n",
        "    qk_t = q @ k_t                                  # -> (B, N, N)\n",
        "    qk_t_normalized = qk_t / math.sqrt(self.dim)    # -> (B, N, N)\n",
        "    attn_weights = F.softmax(qk_t_normalized, dim=-1)\n",
        "\n",
        "    # Weighted sum of values\n",
        "    out = attn_weights @ v\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd3LZ7o1BKpm",
        "outputId": "c9d3a3de-6529-4730-9159-66c63931d5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 16])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 5, 16)  # -> (2, 5, 16)\n",
        "attn = Attention(dim=16)\n",
        "x = attn(x)\n",
        "print(x.shape)             # -> (2, 5, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 Multi‑Head Self‑Attention: Step‑by‑Step\n",
        "\n",
        "**Core idea**  \n",
        "Instead of computing a single attention distribution, multi‑head attention splits the embedding into multiple smaller subspaces (heads), applies attention in each subspace independently, and then combines the results. This allows the model to **capture diverse patterns** of interaction in parallel—e.g., short‑term vs. long‑term dependencies, or syntactic vs. semantic roles.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Input\n",
        "$$\n",
        "X \\;\\in\\; \\mathbb{R}^{B \\times N \\times D},\n",
        "$$\n",
        "where  \n",
        "- $B$ = batch size  \n",
        "- $N$ = number of tokens (sequence length)  \n",
        "- $D$ = total embedding dimension (must be divisible by number of heads $H$)\n",
        "\n",
        "Let $d = D / H$ be the dimension per head.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Linear projections\n",
        "We learn combined projection weights:\n",
        "$$\n",
        "W^{QKV} \\;\\in\\; \\mathbb{R}^{D \\times 3D}\n",
        "$$\n",
        "\n",
        "Split the output into queries, keys, and values:\n",
        "$$\n",
        "Q,\\;K,\\;V = X \\cdot W^{QKV} \\quad\\in\\; \\mathbb{R}^{B \\times N \\times 3D}\n",
        "$$\n",
        "\n",
        "Then reshape to split into $H$ heads:\n",
        "$$\n",
        "Q,\\;K,\\;V \\in \\mathbb{R}^{B \\times H \\times N \\times d}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Compute scaled dot-product attention (per head)\n",
        "$$\n",
        "\\mathrm{Scores} = \\frac{Q \\cdot K^\\top}{\\sqrt{d}} \\quad\\in\\; \\mathbb{R}^{B \\times H \\times N \\times N}\n",
        "$$\n",
        "\n",
        "Apply softmax across the last dimension:\n",
        "$$\n",
        "A = \\mathrm{softmax}(\\mathrm{Scores}) \\quad\\in\\; \\mathbb{R}^{B \\times H \\times N \\times N}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Weighted sum of values\n",
        "$$\n",
        "Z = A \\cdot V \\quad\\in\\; \\mathbb{R}^{B \\times H \\times N \\times d}\n",
        "$$\n",
        "\n",
        "Then concatenate the heads:\n",
        "$$\n",
        "Z_{\\text{concat}} \\in \\mathbb{R}^{B \\times N \\times D}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Final projection\n",
        "Apply a final learned projection to mix the information from all heads:\n",
        "$$\n",
        "\\text{Output} = Z_{\\text{concat}} \\cdot W^O, \\quad W^O \\in \\mathbb{R}^{D \\times D}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Complexity\n",
        "Let $H$ be the number of heads:\n",
        "- **Projections**: $\\mathcal{O}(N D^2)$  \n",
        "- **Attention per head**: $\\mathcal{O}(N^2 d)$ × $H$ = $\\mathcal{O}(N^2 D)$  \n",
        "- **Total**: $\\displaystyle \\mathcal{O}(N^2 D + N D^2)$\n",
        "\n",
        "> Like single-head attention, multi-head attention still scales quadratically with sequence length $N$, but it allows the model to learn richer patterns through head diversity.\n"
      ],
      "metadata": {
        "id": "C3EFxWP6Hs-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "import math\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.dim = dim\n",
        "    self.dk = dim // num_heads\n",
        "    self.qkv = nn.Linear(dim, 3*dim)\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Apply multi-head self-attention mechanism on the input tensor.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (B, N, D), where:\n",
        "            - B = batch size\n",
        "            - N = number of tokens (sequence length or patches)\n",
        "            - D = embedding dimension (must be divisible by num_heads)\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor of shape (B, N, D) after applying multi-head self-attention\n",
        "                and combining all heads. Each token's representation is updated by attending\n",
        "                to all other tokens in the sequence.\n",
        "\n",
        "    Steps:\n",
        "        1. Project input into concatenated queries, keys, and values.\n",
        "        2. Split into multiple heads and compute scaled dot-product attention per head.\n",
        "        3. Concatenate outputs from all heads.\n",
        "        4. Apply final linear projection to combine attention outputs.\n",
        "    \"\"\"\n",
        "    B, N, D = x.shape\n",
        "    qkv = self.qkv(x)                                     # -> (B, N, 3*D)\n",
        "    qkv = qkv.reshape(B, N, 3, self.num_heads, self.dk)   # -> (B, N, 3, Nb_heads, head_dim)\n",
        "    qkv = qkv.permute(2, 0, 3, 1, 4)                      # -> (3, B, Nb_heads, N, head_dim)\n",
        "\n",
        "    # Calculate query, key & value\n",
        "    q, k, v = qkv [0], qkv[1], qkv[2]                     # -> (B, Nb_heads, N, head_dim)\n",
        "\n",
        "    # Compute attention weights\n",
        "    k_t = k.transpose(-1, -2)\n",
        "    qk_t = q @ k_t\n",
        "    scores = qk_t / math.sqrt(self.dk)\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Compute output per head\n",
        "    out = attn_weights @ v                                # -> (B, Nb_heads, N, head_dim)\n",
        "\n",
        "    # Concatenate for all heads\n",
        "    out = out.permute(0, 2, 1, 3)                         # -> (B, N, Nb_heads, head_dim)\n",
        "    out = out.reshape(B, N, D)                            # -> (B, N, D)\n",
        "\n",
        "    # Project in order to combine all information from all heads into a meaningful representation\n",
        "    out = self.proj(out)                                  # -> (B , N, D)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "p_ilAoBq86RP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "x = torch.randn(size=(1, 4, 10))\n",
        "print(x.shape)\n",
        "mhsa = MultiheadSelfAttention(dim = 10, num_heads=2)\n",
        "x = mhsa(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_01iA24r9BoP",
        "outputId": "0c3daa12-1ef9-4305-b252-17ef63f99c36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔧 Transformer Encoder Layer: Step-by-Step\n",
        "\n",
        "**Core idea**  \n",
        "A **Transformer layer** refines token representations using two key operations:\n",
        "1. A **multi-head self-attention block** that mixes information across tokens.\n",
        "2. A **feedforward MLP block** that processes each token independently.\n",
        "\n",
        "Each block is wrapped with **layer normalization** and a **residual connection**, enabling stable and deep training.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Input\n",
        "$$\n",
        "X \\;\\in\\; \\mathbb{R}^{B \\times N \\times D},\n",
        "$$\n",
        "where  \n",
        "- $B$ = batch size  \n",
        "- $N$ = sequence length  \n",
        "- $D$ = embedding dimension\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Multi-Head Self-Attention (MHSA)\n",
        "\n",
        "Apply **layer normalization** before attention:\n",
        "$$\n",
        "X_{\\text{norm1}} = \\text{LayerNorm}(X)\n",
        "$$\n",
        "\n",
        "Apply **multi-head self-attention**:\n",
        "$$\n",
        "Z = \\text{MultiHeadSelfAttention}(X_{\\text{norm1}})\n",
        "\\quad\\in\\; \\mathbb{R}^{B \\times N \\times D}\n",
        "$$\n",
        "\n",
        "Add residual connection:\n",
        "$$\n",
        "X_{\\text{attn}} = X + Z\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Feedforward MLP\n",
        "\n",
        "Apply **layer normalization** again:\n",
        "$$\n",
        "X_{\\text{norm2}} = \\text{LayerNorm}(X_{\\text{attn}})\n",
        "$$\n",
        "\n",
        "Pass through **two-layer MLP with GELU**:\n",
        "$$\n",
        "\\text{MLP}(x) = W_2\\,\\text{GELU}(W_1\\,x) \\quad \\text{with}\\quad W_1 \\in \\mathbb{R}^{D \\times D'},\\; W_2 \\in \\mathbb{R}^{D' \\times D}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "$$\n",
        "Y = \\text{MLP}(X_{\\text{norm2}}) \\quad\\in\\; \\mathbb{R}^{B \\times N \\times D}\n",
        "$$\n",
        "\n",
        "Add second residual connection:\n",
        "$$\n",
        "\\text{Output} = X_{\\text{attn}} + Y\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Summary: The Full Transformer Layer\n",
        "\n",
        "```python\n",
        "x = x + MultiHeadSelfAttention(LayerNorm(x))\n",
        "x = x + MLP(LayerNorm(x))\n"
      ],
      "metadata": {
        "id": "x5W9rii4m7_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.attn = MultiheadSelfAttention(dim=dim, num_heads=num_heads)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    hidden_dim = int (dim * mlp_ratio)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      Transformer encoder block with:\n",
        "      - Pre-LN + Multi-Head Self-Attention\n",
        "      - Pre-LN + MLP\n",
        "      - Two residual connections\n",
        "\n",
        "      Args:\n",
        "          x (Tensor): Input of shape (B, N, D)\n",
        "\n",
        "      Returns:\n",
        "          Tensor of shape (B, N, D)\n",
        "      \"\"\"\n",
        "    # Multi-head self-attention with residual connection\n",
        "    x = x + self.attn(self.norm1(x))                      # -> (B, N, D)\n",
        "\n",
        "    # Feed-forward network with residual connection\n",
        "    x = x + self.mlp(self.norm2(x))                       # -> (B, N, D)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "LThv_0gGm_aA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "x = torch.randn(size=(1, 5, 512))\n",
        "block = Block(dim=512, num_heads=4)\n",
        "x = block(x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2XTq0cj8f6-",
        "outputId": "1a33d9c0-17d0-4d82-f28a-8140cb024602"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 512])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}