{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is a fundamental technique used in deep learning to stabilize and accelerate training.  \n",
        "\n",
        "### ðŸ” Why Normalize?\n",
        "\n",
        "- Prevents **internal covariate shift** by keeping activations within a stable range.\n",
        "- Helps gradients flow more reliably through the network.\n",
        "- Leads to **faster convergence**, **better generalization**, and **less sensitivity** to hyperparameters.\n",
        "\n",
        "### ðŸ§  Two Common Types\n",
        "\n",
        "| Type         | Normalizes Across              | Commonly Used In        |\n",
        "|--------------|--------------------------------|--------------------------|\n",
        "| BatchNorm    | Across the batch               | CNNs, vision models      |\n",
        "| LayerNorm    | Across features (per sample)   | Transformers, NLP        |\n",
        "\n",
        "We'll now implement both `BatchNorm1d` and `LayerNorm` to understand how they work.\n",
        "\n",
        "*Note :* More details about normalization can be found in the corresponding PDF file, where the topic is covered in **greater depth**.\n"
      ],
      "metadata": {
        "id": "i7oJ78sgnUIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Batch Normalization"
      ],
      "metadata": {
        "id": "eJ9BAztSnXa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpSNkBmbnO-h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def manual_batch_norm(x, gamma, beta, eps=1e-5):\n",
        "  \"\"\"\n",
        "  x     : input tensor of shape (batch_size, num_features)\n",
        "  gamma : learnable scale of shape (num_features)\n",
        "  beta  : learnable shift of shape (num_features)\n",
        "  eps   : constant to avoid division by zero -> numerical stability\n",
        "\n",
        "  retunrns normalized tensor of shape (batch_size, num_features)\"\"\"\n",
        "\n",
        "  # Step 1 : Compute Mean and Variance per feature (across the batch -> axis = 0)\n",
        "  mean = x.mean(dim=0, keepdim=True)\n",
        "  var = x.var(dim=0, unbiased=False)\n",
        "\n",
        "  # Step 2 : Normalize\n",
        "  x_normalized = (x - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "  # Step 3 : Scale & Shift\n",
        "  out = gamma * x_normalized + beta\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "x = torch.tensor(\n",
        "    [[1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6]], dtype=float\n",
        ")\n",
        "\n",
        "gamma = torch.tensor([1, -2], dtype=float) # scale\n",
        "beta = torch.tensor([0, 0], dtype=float) # shift\n",
        "\n",
        "out = manual_batch_norm(x, gamma, beta)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG_8rPVKnlyu",
        "outputId": "ff4c76e8-63f8-4bbb-bc5d-78aa244aa79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2247,  2.4495],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.2247, -2.4495]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Layer Normalization"
      ],
      "metadata": {
        "id": "cIhsvOiknnkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_layer_normalization(x, gamma, beta, eps=1e-5):\n",
        "  \"\"\"\n",
        "  x       : input tensor of shape (batch_size, num_features)\n",
        "  gamma   : learnable scale of shape (num_features)\n",
        "  beta    : learnable shift of shape (num_features)\n",
        "  eps     : constant to avoid division by zero -> numerical stability\n",
        "\n",
        "  return normalized tensor of shape (batch_size, num_features)\"\"\"\n",
        "\n",
        "  # Step 1 : Calculate mean and variance per sample (across the features of the same sample)\n",
        "  mean = x.mean(dim=1, keepdim=True)\n",
        "  variance = x.var(dim=1, keepdim=True)\n",
        "\n",
        "  # Step 2 : Normalize\n",
        "  x_hat = (x - mean) / torch.sqrt(variance + eps)\n",
        "\n",
        "  # Step 3 : Scale & Shift\n",
        "  out = gamma * x_hat + beta\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "y1F_vFUpnrf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "x = torch.tensor(\n",
        "    [[1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]], dtype=float\n",
        ")\n",
        "\n",
        "gamma = torch.ones(3)\n",
        "beta = torch.zeros(3)\n",
        "\n",
        "out = manual_layer_normalization(x, gamma, beta)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "id": "ehkzSm-sn1wO",
        "outputId": "9f0e9214-826a-48fc-b283-a1ceec1983b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0000,  0.0000,  1.0000],\n",
            "        [-1.0000,  0.0000,  1.0000],\n",
            "        [-1.0000,  0.0000,  1.0000],\n",
            "        [-1.0000,  0.0000,  1.0000]], dtype=torch.float64)\n"
          ]
        }
      ]
    }
  ]
}